{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "bert_classifier_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6hrrdf2wJLf"
      },
      "source": [
        "# BERT Classifier baseline\n",
        "\n",
        "`Author: YUAN Yanzhe`\n",
        "\n",
        "\n",
        "- BERT+Linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYWyAfTOwLH7",
        "outputId": "2e230a4b-c640-4a2a-d33d-99c3ee8c4658"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOGxDBMGwbCA"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/6000_proj1')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKnxFCagw0Q3",
        "outputId": "981ba060-49e7-4623-dc11-b0fa7873b030"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6KIa104wJLk",
        "outputId": "829bd40a-867c-4e6f-f084-e91f36556f3a"
      },
      "source": [
        "# machine laerning packages\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# deep learning packages\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# settings\n",
        "stopwords = set(stopwords.words(\"english\"))    \n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Yk7X7rwJLl"
      },
      "source": [
        "## 1. Data Proprocessing\n",
        "\n",
        "- lower word\n",
        "- re filtering\n",
        "- stopword filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbHLHP5iwJLl"
      },
      "source": [
        "# preprocessing\n",
        "\n",
        "def load_data(file_name):\n",
        "    df = pd.read_csv(file_name)\n",
        "    return df, df['text'], df['context'], df['impact_label']\n",
        "\n",
        "def preprocessing(text, lower_word=True, re_word=True, stop_word=True):\n",
        "    # input: text:str\n",
        "    # output: preprocessed sentence:str\n",
        "    \n",
        "    # lower_word\n",
        "    if lower_word:\n",
        "        text = text.lower()\n",
        "    \n",
        "    # re_word\n",
        "    if re_word:\n",
        "         text = text.strip('[]').replace(\"'\", \"\").replace(\"'\", \"\").replace(\"%\", \" percent \")\\\n",
        "                    .replace(\"$\", \" dollar \").replace('&', ' and ').replace(\"-\", \" \")\n",
        "                    #.replace('.','').replace('\"', \"\")    \n",
        "    # stop_word\n",
        "    if stop_word:\n",
        "        text = ' '.join([word for word in text.split() if word not in stopwords])\n",
        "        # remove whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "lZj8JMugwJLl",
        "outputId": "d699826a-be0a-44bc-e6ec-7c9136964a8f"
      },
      "source": [
        "# Load Data\n",
        "train_raw, train_raw_text, train_raw_context, train_raw_label = load_data('data/train.csv')\n",
        "valid_raw, valid_raw_text, valid_raw_context, valid_raw_label = load_data('data/valid.csv')\n",
        "test_raw, test_raw_text, test_raw_context, test_raw_label = load_data('data/test.csv')\n",
        "\n",
        "train_raw.head()\n",
        "#valid_raw.head()\n",
        "#test_raw.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>context</th>\n",
              "      <th>stance_label</th>\n",
              "      <th>impact_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_0</td>\n",
              "      <td>All adoption agencies conduct interviews and h...</td>\n",
              "      <td>['A license should be required in order to hav...</td>\n",
              "      <td>['NULL', 'OPPOSE', 'OPPOSE', 'SUPPORT']</td>\n",
              "      <td>MEDIUM_IMPACT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_1</td>\n",
              "      <td>Genetically modified crops need fewer scarce r...</td>\n",
              "      <td>['The sale of genetically modified food should...</td>\n",
              "      <td>['NULL', 'OPPOSE', 'SUPPORT']</td>\n",
              "      <td>IMPACTFUL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_2</td>\n",
              "      <td>Some of Antifa's actions may be morally questi...</td>\n",
              "      <td>['Democrats should not cooperate with Donald T...</td>\n",
              "      <td>['NULL', 'OPPOSE', 'SUPPORT', 'OPPOSE', 'SUPPO...</td>\n",
              "      <td>MEDIUM_IMPACT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_3</td>\n",
              "      <td>Most pet lovers eat animals.</td>\n",
              "      <td>['People should donate to organisations that s...</td>\n",
              "      <td>['NULL', 'OPPOSE', 'OPPOSE', 'OPPOSE', 'SUPPORT']</td>\n",
              "      <td>MEDIUM_IMPACT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_4</td>\n",
              "      <td>Biologically-driven reproductive desires are c...</td>\n",
              "      <td>['Gender (the concept of masculinity and femin...</td>\n",
              "      <td>['NULL', 'SUPPORT', 'SUPPORT', 'OPPOSE', 'SUPP...</td>\n",
              "      <td>IMPACTFUL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ...   impact_label\n",
              "0  train_0  ...  MEDIUM_IMPACT\n",
              "1  train_1  ...      IMPACTFUL\n",
              "2  train_2  ...  MEDIUM_IMPACT\n",
              "3  train_3  ...  MEDIUM_IMPACT\n",
              "4  train_4  ...      IMPACTFUL\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02jrKyrUwJLm",
        "outputId": "390c34d3-6913-47c7-b9ef-316a0ba41cdb"
      },
      "source": [
        "# Do Preprocessing\n",
        "# train and valid data\n",
        "train_text = [preprocessing(text, lower_word=True, re_word=True, stop_word=True) for text in train_raw_text]\n",
        "valid_text = [preprocessing(text, lower_word=True, re_word=True, stop_word=True) for text in valid_raw_text]\n",
        "train_context = [preprocessing(text, lower_word=True, re_word=True, stop_word=True) for text in train_raw_context]\n",
        "valid_context = [preprocessing(text, lower_word=True, re_word=True, stop_word=True) for text in valid_raw_context]\n",
        "\n",
        "class_to_label = {'NOT_IMPACTFUL':0, 'MEDIUM_IMPACT':1, 'IMPACTFUL':2}\n",
        "train_label = train_raw_label.map(class_to_label).tolist()\n",
        "valid_label = valid_raw_label.map(class_to_label).tolist()\n",
        "\n",
        "print(len(train_text))\n",
        "print(len(train_context))\n",
        "print(len(train_label))\n",
        "print(len(valid_text))\n",
        "print(len(valid_context))\n",
        "print(len(valid_label))\n",
        "\n",
        "# test data\n",
        "test_text = [preprocessing(text, lower_word=True, re_word=True, stop_word=True) for text in test_raw_text]\n",
        "test_context = [preprocessing(text, lower_word=True, re_word=True, stop_word=True) for text in test_raw_context]\n",
        "test_label = test_raw_label.map({'UNKNOWN':-1})\n",
        "print(len(test_text))\n",
        "print(len(test_context))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5170\n",
            "5170\n",
            "5170\n",
            "1108\n",
            "1108\n",
            "1108\n",
            "1108\n",
            "1108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzYPRSvqcmdI"
      },
      "source": [
        "## 2. EDA\n",
        "\n",
        "- class imbalance\n",
        "- max_length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TpvIFtjwJLm",
        "outputId": "e0f7107e-36a5-451b-ff2c-898d4bb5bd6a"
      },
      "source": [
        "# See Stats\n",
        "print('The shape of train data: ', train_raw.shape)\n",
        "print('The shape of valid data: ', valid_raw.shape)\n",
        "#print('The shape of test data: ', test_raw.shape)\n",
        "print('\\n# of NULL value in train data:\\n', train_raw.isnull().sum())\n",
        "print('# of NULL value in valid data:\\n', valid_raw.isnull().sum())\n",
        "#print('# of NULL value in valid data:\\n', test_raw.isnull().sum())\n",
        "\n",
        "train_imbalance = train_raw.impact_label.value_counts()\n",
        "valid_imbalance = valid_raw.impact_label.value_counts()\n",
        "print('\\nimbalance in train data\\n', train_imbalance)\n",
        "print('imbalance in valid data\\n', valid_imbalance)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of train data:  (5170, 5)\n",
            "The shape of valid data:  (1108, 5)\n",
            "\n",
            "# of NULL value in train data:\n",
            " id              0\n",
            "text            0\n",
            "context         0\n",
            "stance_label    0\n",
            "impact_label    0\n",
            "dtype: int64\n",
            "# of NULL value in valid data:\n",
            " id              0\n",
            "text            0\n",
            "context         0\n",
            "stance_label    0\n",
            "impact_label    0\n",
            "dtype: int64\n",
            "\n",
            "imbalance in train data\n",
            " IMPACTFUL        3021\n",
            "NOT_IMPACTFUL    1126\n",
            "MEDIUM_IMPACT    1023\n",
            "Name: impact_label, dtype: int64\n",
            "imbalance in valid data\n",
            " IMPACTFUL        641\n",
            "NOT_IMPACTFUL    252\n",
            "MEDIUM_IMPACT    215\n",
            "Name: impact_label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Y7YzriAqwJLn",
        "outputId": "4989d26f-5f78-4d1c-aba5-fea9beadceb4"
      },
      "source": [
        "test_raw.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>context</th>\n",
              "      <th>stance_label</th>\n",
              "      <th>impact_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test_0</td>\n",
              "      <td>['Cloning animals advances science in that sci...</td>\n",
              "      <td>['Cloning animals is ethical', 'The possible s...</td>\n",
              "      <td>['NULL', 'SUPPORT', 'SUPPORT']</td>\n",
              "      <td>UNKNOWN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test_1</td>\n",
              "      <td>['Propaganda was widely used before the 20th c...</td>\n",
              "      <td>['Europeans, defined as citizens of what today...</td>\n",
              "      <td>['NULL', 'OPPOSE', 'SUPPORT', 'OPPOSE']</td>\n",
              "      <td>UNKNOWN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test_2</td>\n",
              "      <td>['Polygamous marriages bear a high likelihood ...</td>\n",
              "      <td>['Polygamy should be legal', 'Polygamy can hav...</td>\n",
              "      <td>['NULL', 'SUPPORT', 'SUPPORT']</td>\n",
              "      <td>UNKNOWN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test_3</td>\n",
              "      <td>['White supremacists threaten and endanger a v...</td>\n",
              "      <td>['Internet companies are wrong in denying serv...</td>\n",
              "      <td>['NULL', 'OPPOSE']</td>\n",
              "      <td>UNKNOWN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test_4</td>\n",
              "      <td>['Voters would be able to better determine the...</td>\n",
              "      <td>['Abolishing all aspects of privacy (through a...</td>\n",
              "      <td>['NULL', 'SUPPORT', 'SUPPORT']</td>\n",
              "      <td>UNKNOWN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... impact_label\n",
              "0  test_0  ...      UNKNOWN\n",
              "1  test_1  ...      UNKNOWN\n",
              "2  test_2  ...      UNKNOWN\n",
              "3  test_3  ...      UNKNOWN\n",
              "4  test_4  ...      UNKNOWN\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pd2OllQwJLn",
        "outputId": "c1ce9383-bac4-4cc7-dbfb-8a7141c1dbe0"
      },
      "source": [
        "# find MAX_LENGTH\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "all_data = train_context+train_text\\\n",
        "            +valid_context+valid_text\\\n",
        "            +test_context+test_text\n",
        "sents_encode = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_data]\n",
        "sent_lens = [len(sent) for sent in sents_encode]\n",
        "print('Max length: ', max(sent_lens))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length:  442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "KAWBQqj1wJLo",
        "outputId": "dc48c39b-b59c-4da9-e398-b75259787571"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.histplot(sent_lens)\n",
        "plt.xlim([0,512])\n",
        "plt.xlabel('Sent Len');"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIEskoHowJLn"
      },
      "source": [
        "## 2. Model: BERT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjv4_gK2wJLo"
      },
      "source": [
        "### 2.1 bert pipeline\n",
        "\n",
        "- Load Data\n",
        "  - class BertDataPreprocessor: to make all data into BERT input.\n",
        "  - def data_loader: to transfer datasets into the form of PyTorch DataLoader.\n",
        "- Define Model\n",
        "  - BERT (pooler_output/last_hidden_state)\n",
        "  - dropout\n",
        "  - linear\n",
        "- Train Model\n",
        "  - train and eval\n",
        "  - train on the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArPoPhmpwJLo"
      },
      "source": [
        "# Hyperparameters: for tuning\n",
        "\n",
        "PRETRAINED_MODEL = 'bert-base-uncased'\n",
        "MAX_LENGTH = 256  # 512\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "\n",
        "bert_model = BertModel.from_pretrained(PRETRAINED_MODEL)\n",
        "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3MbebNbwJLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7cc1a9f-3931-4334-9ff3-1b106e381fc5"
      },
      "source": [
        "# Data Loader: transfer data into BERT input\n",
        "\n",
        "class BertDataPreprocessor():\n",
        "    def __init__(self, text, context, label, tokenizer, max_len):\n",
        "        self.text = text\n",
        "        self.context = context\n",
        "        self.label = label\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        context = self.context[item]\n",
        "        text = self.text[item]\n",
        "        label = self.label[item]\n",
        "        \n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            #context,\n",
        "            text,\n",
        "            context,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        \n",
        "        return {'context':context, 'text':text, 'input_ids': encoding['input_ids'].flatten(),\n",
        "                'attention_mask': encoding['attention_mask'].flatten(),\n",
        "                'label': torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "\n",
        "def data_loader(text, context, label, tokenizer, max_len, batch_size):\n",
        "    data = BertDataPreprocessor(\n",
        "        text=text,\n",
        "        context=context,\n",
        "        label=label,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "    \n",
        "    return DataLoader(data, batch_size=batch_size,num_workers=4)\n",
        "\n",
        "\n",
        "# ---Train Model 1.0: train and eval---\n",
        "train_data_loader = data_loader(train_text, train_context, train_label, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
        "valid_data_loader = data_loader(valid_text, valid_context, valid_label, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
        "test_data_loader = data_loader(test_text, test_context, test_label, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
        "\n",
        "\n",
        "# ---Train Model 2.0: train the whole dataset---\n",
        "# train_text_full = train_text + valid_text\n",
        "# train_context_full = train_context + valid_context\n",
        "# train_label_full = train_label + valid_label\n",
        "\n",
        "# train_data_loader = data_loader(train_text_full, train_context_full, train_label_full, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
        "# test_data_loader = data_loader(test_text, test_context, test_label, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynT7p4NPwJLp",
        "outputId": "512186f9-9d5d-40d8-f962-9153881cccbe"
      },
      "source": [
        "# check output\n",
        "data = next(iter(train_data_loader))\n",
        "data.keys()\n",
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['label'].shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 256])\n",
            "torch.Size([32, 256])\n",
            "torch.Size([32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpAQ-4GiwJLp"
      },
      "source": [
        "# Define Model\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_model, num_class):\n",
        "        super(BertClassifier,self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, num_class)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        res = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        res = self.dropout(res['last_hidden_state'][:,0,:])  # last_hidden_state[:,0,:], res['pooler_output']\n",
        "        res = self.linear(res)\n",
        "        return res\n",
        "\n",
        "# settings\n",
        "net = BertClassifier(bert_model, len(class_to_label)).to(device)\n",
        "#print(net)\n",
        "\n",
        "optimizor = AdamW(net.parameters(), lr=2e-5, correct_bias=False)\n",
        "loss_func = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer=optimizor, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD8NS-gnwJLq"
      },
      "source": [
        "def train_model(net, data_loader, optimizor, loss_func, scheduler, num_data, device):\n",
        "    net = net.train()  # open up dropout and normalization\n",
        "    preds, total_l, correct = [], [], 0\n",
        "    \n",
        "    for data in data_loader:\n",
        "        # data\n",
        "        input_ids, attention_mask = data['input_ids'].to(device), data['attention_mask'].to(device)\n",
        "        y = data['label'].to(device)\n",
        "        \n",
        "        # pipeline\n",
        "        output = net(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, pred = torch.max(output,dim=1)  # values, indices = torch.max(), use indices to reflect pred_labels\n",
        "        loss = loss_func(output,y)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "        \n",
        "        optimizor.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        optimizor.zero_grad()\n",
        "        \n",
        "        # record stats\n",
        "        preds.append(pred)\n",
        "        correct += torch.sum(pred==y)\n",
        "        total_l.append(loss.item())\n",
        "\n",
        "        \n",
        "    return correct.double()/num_data, np.mean(total_l), torch.cat(preds, dim=0).cpu()\n",
        "\n",
        "def eval_model(net, data_loader, loss_func, num_data, device):\n",
        "    net = net.eval() \n",
        "    preds, total_l, correct = [], [], 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            # data\n",
        "            input_ids, attention_mask = data['input_ids'].to(device), data['attention_mask'].to(device)\n",
        "            y = data['label'].to(device)\n",
        "        \n",
        "            # pipeline: just foward\n",
        "            output = net(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, pred = torch.max(output,dim=1)  \n",
        "            loss = loss_func(output,y)\n",
        "        \n",
        "            # record stats\n",
        "            preds.append(pred)\n",
        "            correct += torch.sum(pred==y)\n",
        "            total_l.append(loss.item())\n",
        "    \n",
        "    return correct.double()/num_data, np.mean(total_l), torch.cat(preds, dim=0).cpu()\n",
        "\n",
        "def test_model(net, data_loader):\n",
        "    net = net.eval()\n",
        "    preds, pred_probs = [],[]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            # data\n",
        "            input_ids, attention_mask = data['input_ids'].to(device), data['attention_mask'].to(device)\n",
        "            y = data['label'].to(device)\n",
        "            \n",
        "            # pipeline\n",
        "            output = net(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, pred = torch.max(output,dim=1)\n",
        "            \n",
        "            preds.extend(pred.cpu())\n",
        "            #pred_prob = nn.functional.softmax(output, dim=1)\n",
        "            #pred_probs.extend(pred_prob)\n",
        "            \n",
        "        predictions = torch.stack(preds).cpu().numpy()\n",
        "        #prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "        return predictions"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7-RDIkgwJLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9433c01-2088-4859-f64d-49c7cb2715a2"
      },
      "source": [
        "# Train Model 1.0: train and eval \n",
        "\n",
        "train_history, best_accuracy, best_f1 = defaultdict(list), 0, 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # tain\n",
        "    train_acc, train_loss, train_preds = train_model(net, train_data_loader, optimizor, loss_func,\\\n",
        "                                        scheduler, len(train_text), device)\n",
        "    # show stats\n",
        "    train_f1 = f1_score(train_label, train_preds, average='macro')\n",
        "    print('--------------')\n",
        "    print('Epoch: %d/%d \\n Train_loss: %f, Train_acc: %.4f%%, Train_F1_Macro: %f'\\\n",
        "          %((epoch+1), EPOCHS, train_loss, train_acc*100, train_f1))\n",
        "    \n",
        "    # validate\n",
        "    valid_acc, valid_loss, valid_preds = eval_model(net, valid_data_loader, loss_func, len(valid_text), device)\n",
        "    # show stats\n",
        "    valid_f1 = f1_score(valid_label, valid_preds, average='macro')\n",
        "    print('Epoch: %d/%d \\n Vraid_loss: %f, Vraid_acc: %.4f%%, Valid_F1_Macro: %f'\\\n",
        "          %((epoch+1), EPOCHS, valid_loss, valid_acc*100, valid_f1))\n",
        "    \n",
        "    train_history['train_acc'].append(train_acc)\n",
        "    train_history['train_f1'].append(train_f1)\n",
        "    train_history['train_loss'].append(train_loss)\n",
        "    train_history['val_acc'].append(valid_acc)\n",
        "    train_history['valid_f1'].append(valid_f1)\n",
        "    train_history['val_loss'].append(valid_loss)\n",
        "\n",
        "    if valid_f1 > best_f1:\n",
        "        torch.save(net.state_dict(), 'model_saved/bert_clf_test_1.h')\n",
        "        best_f1 = valid_f1\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------\n",
            "Epoch: 1/5 \n",
            " Train_loss: 0.869011, Train_acc: 60.9478%, Train_F1_Macro: 0.424716\n",
            "Epoch: 1/5 \n",
            " Vraid_loss: 0.794685, Vraid_acc: 63.2671%, Valid_F1_Macro: 0.517993\n",
            "--------------\n",
            "Epoch: 2/5 \n",
            " Train_loss: 0.753658, Train_acc: 64.4101%, Train_F1_Macro: 0.509900\n",
            "Epoch: 2/5 \n",
            " Vraid_loss: 0.801706, Vraid_acc: 61.3718%, Valid_F1_Macro: 0.543075\n",
            "--------------\n",
            "Epoch: 3/5 \n",
            " Train_loss: 0.658984, Train_acc: 69.4778%, Train_F1_Macro: 0.615848\n",
            "Epoch: 3/5 \n",
            " Vraid_loss: 0.806527, Vraid_acc: 63.0866%, Valid_F1_Macro: 0.569012\n",
            "--------------\n",
            "Epoch: 4/5 \n",
            " Train_loss: 0.538313, Train_acc: 76.2282%, Train_F1_Macro: 0.717095\n",
            "Epoch: 4/5 \n",
            " Vraid_loss: 0.886037, Vraid_acc: 64.8917%, Valid_F1_Macro: 0.578656\n",
            "--------------\n",
            "Epoch: 5/5 \n",
            " Train_loss: 0.440620, Train_acc: 82.2050%, Train_F1_Macro: 0.790899\n",
            "Epoch: 5/5 \n",
            " Vraid_loss: 0.921278, Vraid_acc: 65.3430%, Valid_F1_Macro: 0.594540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LQ3oYO372JR"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOu2CH_fwJLr"
      },
      "source": [
        "# # Train Model 2.0: train the whole dataset (train+valid) \n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#     # tain\n",
        "#     train_acc, train_loss, train_preds = train_model(net, train_data_loader, optimizor, loss_func,\\\n",
        "#                                         scheduler, len(train_text_full), device)\n",
        "#     # show stats\n",
        "#     print('--------------')\n",
        "#     train_f1 = f1_score(train_label, train_preds, average='macro')\n",
        "# print('Epoch: %d/%d \\n Train_loss: %f, Train_acc: %.4f%%, Train_F1_Macro: %f'\\\n",
        "#         %((epoch+1), EPOCHS, train_loss, train_acc*100, train_f1))\n",
        "      \n",
        "#torch.save(net.state_dict(), 'model_saved/bert_clf_0.h')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSIVdHTyAr7W"
      },
      "source": [
        "# Save Model\n",
        "# torch.save(net.state_dict(), 'model_saved/bert_clf_0.h')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Hl6iup7oO8"
      },
      "source": [
        "### 2.2 predict and submit\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6b_EEe47mAK"
      },
      "source": [
        "# Make Predictions\n",
        "\n",
        "# net = BertClassifier(bert_model, len(class_to_label))\n",
        "# net.load_state_dict(torch.load('model_saved/bert_clf_0.h'))\n",
        "# net = net.to(device)\n",
        "\n",
        "# predictions = test_model(net,test_data_loader)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ie-u83770k1"
      },
      "source": [
        "# check output\n",
        "# print(len(predictions),type(predictions))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNeXXlxBzQan"
      },
      "source": [
        "# Submit\n",
        "\n",
        "# submission = pd.read_csv('data/submission_sample.csv')\n",
        "# submission['pred'] = pd.Series(predictions)\n",
        "# submission.to_csv('submission_4.csv', index=False)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "794yj3LizQkA"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}